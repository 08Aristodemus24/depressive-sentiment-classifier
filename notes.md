* works iwth conda with python 3.11.8
* Yes, you are exactly right. The az login command you run in your terminal is the most common way that Terraform, when configured with the azurerm provider, is able to access and manage your Azure resources.

Here's a breakdown of how that works:

az login and Authentication: When you run az login (using the Azure CLI), you are authenticating your local machine with your Azure account. This command generates an access token and stores it locally in a cache.

azurerm Provider and the Azure CLI: The azurerm provider for Terraform is designed to work seamlessly with the Azure CLI. By default, it looks for an active, cached access token from a previous az login session.

No Explicit Credentials Needed: Because Terraform finds this cached token, you don't need to provide explicit credentials (like client ID, client secret, or subscription ID) directly within your Terraform code. The provider uses the token to make authenticated API calls to Azure on your behalf.

How Long It Lasts: The token generated by az login has a limited lifespan (typically a few hours). You'll know it's expired when Terraform starts throwing authentication or authorization errors. When that happens, you just need to run az login again to refresh the token.

This method is highly convenient for local development and testing. For automated, non-interactive environments (like in a CI/CD pipeline), you would typically use a Service Principal with a client ID and client secret, as this doesn't require a user to be present to log in.

* kaya gumagana ang pag gawa ng resources ng terraform even if you've not exported the env variables explicitly is because of `az login` because when we login remember that it returns the ff. credentials
```
{
  "environmentName": "AzureCloud",
  "homeTenantId": "2e048c9e-b551-4c29-9888-80fb187ca009",
  "id": "b7d48940-7100-4532-b87c-07db6fa079c9",
  "isDefault": true,
  "managedByTenants": [],
  "name": "Azure subscription 1",
  "state": "Enabled",
  "tenantDefaultDomain": "larrymiguelcuevagmail.onmicrosoft.com",
  "tenantDisplayName": "Default Directory",
  "tenantId": "2e048c9e-b551-4c29-9888-80fb187ca009",
  "user": {
    "name": "larrymiguelcueva@gmail.com",
    "type": "user"
  }
}
```

that we can use specifcially th `SubscriptionID` (id) to create a role based access control for our user or subscription using `az ad sp create-for-rbac --role="Contributor" --scopes="/subscriptions/<SUBSCRIPTION_ID>"`. But even if this weren't the case the reason we are able to have terraform automatically create resources is because when we logged in using azure CLI which prompted us with a UI to login to our account this automatically set the login credentials as cache in the azure CLI which terraform seamlessly interfaces with to retrieve such credentials of the currently logged in user to then make the changes to our azure portal. And we want to show the current account logged in we can use `az account show` and it will return the same credentials as with the `az login` command reprenting the current azure account signed in that azure CLI can use to authenticate to azure portal. But when we log this out using `az logout` we will not anymore be able to run terraform to create azure resources as this will need again access to our account to do so.

So like azure CLI we need to install aws CLI. AWS CLI versions 1 and 2 use the same aws command name mind you so we need to check via aws --version to see if we have version 2.

NOte that we cannot just use any IAM user aws access key id and aws secret access key because if we specifically created such keys with only certain permissions like beign able to have admin access to an S3 bucket then by definition we will only get to read, write, update, delete, list, s3 buckets using this IAM user credentials.

iba siya sa azure because in aws we `create IAM > assign roles/permissions to IAM user` but in azure we `create user > create resource > assign role based access control to user for this resource`.

so in order for terraform to use the credentials we create to create specific aws resources we need to specify the resources that the IAM user we will be created can have access to. E.g. can it read only to s3, but also create an EC2 isntance, write using elastic map reduce (EMR), control aws step function, aws lambda, or aws glue etc. These services we must know in advance whether to grant access to our IAM user in order to create credentials that terraform can use to create.

main.tf
```
provider "aws" {
  region = "ap-southeast-2"
}

resource "aws_s3_bucket" "dsc_bucket" {
  bucket = "${var.project_name}-bucket"

  tags = {
    Name        = "${var.project_name}-bucket"
    Environment = "Dev"
  }
}
```

```
  # aws_s3_bucket.dsc_bucket will be created
  + resource "aws_s3_bucket" "dsc_bucket" {
      + acceleration_status         = (known after apply)
      + acl                         = (known after apply)
      + arn                         = (known after apply)
      + bucket                      = "depressive-sentiment-classifier-bucket"
      + bucket_domain_name          = (known after apply)
      + bucket_prefix               = (known after apply)
      + bucket_regional_domain_name = (known after apply)
      + force_destroy               = false
      + hosted_zone_id              = (known after apply)
      + id                          = (known after apply)
      + object_lock_enabled         = (known after apply)
      + policy                      = (known after apply)
      + region                      = (known after apply)
      + request_payer               = (known after apply)
      + tags                        = {
          + "Environment" = "Dev"
          + "Name"        = "depressive-sentiment-classifier-bucket"
        }
      + tags_all                    = {
          + "Environment" = "Dev"
          + "Name"        = "depressive-sentiment-classifier-bucket"
        }
      + website_domain              = (known after apply)
      + website_endpoint            = (known after apply)

      + cors_rule (known after apply)

      + grant (known after apply)

      + lifecycle_rule (known after apply)

      + logging (known after apply)

      + object_lock_configuration (known after apply)

      + replication_configuration (known after apply)

      + server_side_encryption_configuration (known after apply)

      + versioning (known after apply)

      + website (known after apply)
    }
```

* to resolve 
```
Policy: operation error S3: PutBucketPolicy, https response error StatusCode: 403, RequestID: FCT298GRTQS3Z55F, HostID: fC5pXTesXH5xoeB17xTe6q/kYkuEFnfnKKeygQNqG7qPDEdL4DbZdBdP5TOHgDRkQQOOzS1aQw0=, api error AccessDenied: User: arn:aws:iam::612565766933:user/depressive-sentiment-classifier-admin is not authorized to perform: s3:PutBucketPolicy on resource: "arn:aws:s3:::depressive-sentiment-classifier-bucket" because public policies are blocked by the BlockPublicPolicy block public access setting.
│
│   with aws_s3_bucket_policy.dsc_bucket_access_policy,
│   on main.tf line 14, in resource "aws_s3_bucket_policy" "dsc_bucket_access_policy":
│   14: resource "aws_s3_bucket_policy" "dsc_bucket_access_policy" {
│
╵

Your bucket policy changes can't be saved
You either don't have permissions to edit the bucket policy, or your bucket policy grants a level of public access that conflicts with your Block Public Access settings. To edit a bucket policy, you need the s3:PutBucketPolicy permission. To review which Block Public Access settings are turned on

User: arn:aws:iam::612565766933:root is not authorized to perform: s3:PutBucketPolicy on resource: "arn:aws:s3:::depressive-sentiment-classifier-bucket" because public policies are blocked by the BlockPublicPolicy block public access setting.
```
